<!DOCTYPE html>
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <title></title>
  <link href="../research.css" media="all" rel="stylesheet" type="text/css">
  <script src="../research.js" type="text/javascript"></script>
  <meta content="authenticity_token" name="csrf-param">
<meta content="7TNJEVn066MRFNpaLNRxEEoY88t5iFZlJU5Jh14dmB8=" name="csrf-token">
  <!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->

</head>
<body>
  <div class="container">
    <table>
      <tbody><tr>
	<td>
	    <h4 class="title">
    Grounded Language Learning from Video Described with Sentences
  </h4>
  <p class="authors muted"><b>Haonan Yu</b> and <b>Jeffrey Mark Siskind</b></p>

	  <hr size="1">
	  
	</td>
      </tr>
      <tr>
	<td>
	  <div class="paragraph">
	    <table>
	      <tbody><tr>
		<td>
		    <div align="center"> 
    <img alt="scenario" src="scene.png" height="300" width="600">
  </div>

		</td>
	      </tr>
	      <tr>
		<td>
		  <hr size="1">
		    <h4 class="muted subtitle"> Description </h4>
  <div>
    <p> People learn language through exposure to a rich
      perceptual context. Language is grounded by mapping words,
      phrases, and sentences to meaning representations referring
      to the world. It has shown that even with referential
      uncertainty and noise, a system based
      on <i>cross-situational</i> learning can robustly acquire a
      lexicon, mapping words to word-level meanings from sentences
      paired with sentence-level meanings. We present a method
      that learns representations for word meanings from short
      video clips paired with sentences. Unlike prior work on
      learning language from symbolic input, our input consists of
      video of people interacting with multiple complex objects in
      outdoor environments. Unlike prior computer-vision
      approaches that learn from videos with verb labels or images
      with noun labels, our labels are sentences containing nouns,
      verbs, prepositions, adjectives, and adverbs. The
      correspondence between words and concepts in the video is
      learned in an unsupervised fashion, even when the video
      depicts simultaneous events described by multiple sentences
      or when different aspects of a single event are described
      with multiple sentences. The learned word meanings can be
      subsequently used to automatically generate description of
      new video.
    </p>
  </div>

		</td>
	      </tr>
	      <tr>
		<td>
		  <hr size="1">
		    <h4 class="muted subtitle"> Reference </h4>
  <div>
    <p> H. Yu and J. M. Siskind.  'Grounded Language Learning from
      Video Described with Sentences', In <i>Proceedings of the 51st
      Annual Meeting of the Association for Computational
      Linguistics</i>, 2013, <b>best paper award</b>.
    </p>
    <a href="../../papers/acl2013.pdf" class="btn btn-mini btn-inverse">pdf</a>
    <a href="http://github.com/yu239/sentence-training" class="btn
       btn-mini btn-inverse">source code (scheme,c,c++)</a>
    <a href="http://upplysingaoflun.ecn.purdue.edu/~qobi/acl2013-dataset.tgz" class="btn btn-mini btn-inverse">dataset</a>
    <a href="talk.pdf" class="btn btn-mini btn-inverse">talk file (beamer)</a>
    <a href="talk-script.txt" class="btn btn-mini btn-inverse">talk script (text)</a>
  </div>

		</td>
	      </tr>
	    </tbody></table>
	  </div>
	</td>
      </tr>
    </tbody></table>
    <footer class="footer">
    </footer>

</div>



</body></html>
